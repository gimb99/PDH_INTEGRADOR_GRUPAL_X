# -*- coding: utf-8 -*-
"""TI-CTG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myFma8aLVn_7EGsgCTGpDMcrj-bWgBxl

## ğŸ“¥ Paso 2.1 â€“ Ingesta de documentos PDF y traducciÃ³n automÃ¡tica

Este paso carga los documentos tÃ©cnicos desde el disco local.
Se leen PDFs tanto en espaÃ±ol como en inglÃ©s.  
Los documentos en inglÃ©s se traducen automÃ¡ticamente al espaÃ±ol con un modelo de Hugging Face.
El resultado final es un corpus unificado en espaÃ±ol que se utilizarÃ¡ en los siguientes pasos del sistema RAG.
"""

# ğŸ“Œ Paso 2.1 â€“ Ingesta y traducciÃ³n automÃ¡tica de documentos PDF

# ========================
# âœ… LibrerÃ­as necesarias
# ========================
#     # Install missing library
#!pip install langchain-community pypdf  # Install missing library
#!pip install chromadb
from langchain_community.document_loaders import PyMuPDFLoader  # Para cargar PDFs

from transformers import pipeline   # Para traducir texto
import os   # Para acceder a archivos en disco

# Original
#from langchain.vectorstores import Chroma
# Version 2
from langchain_community.vectorstores import Chroma


# ========================
# âœ… Modelo de traducciÃ³n
# ========================
# Este modelo traduce texto de inglÃ©s a espaÃ±ol usando Hugging Face
translator = pipeline("translation_en_to_es", model="Helsinki-NLP/opus-mt-en-es")

# Paso 2.1 â€“ INGESTA DE DOCUMENTOS: Carga de archivos PDF desde Google Drive

# ======================================
# ğŸ”§ 1. Montamos Google Drive (solo en Colab)
# ======================================
#from google.colab import drive
#drive.mount('/content/drive')

# ======================================
# ğŸ“ 2. Definimos las carpetas del corpus
# ======================================
carpetas_es = "./data/corpus_base"

carpeta_en = "./data/complementos_tecnicos"

# ======================================
# ğŸ“š 3. FunciÃ³n para cargar documentos PDF
# ======================================
#from langchain.document_loaders import PyMuPDFLoader

def cargar_documentos_desde_carpeta(rutas_carpeta):
    documentos = []
    for ruta in rutas_carpeta:
        for archivo in os.listdir(ruta):
            if archivo.endswith(".pdf"):
                ruta_completa = os.path.join(ruta, archivo)
                loader = PyMuPDFLoader(ruta_completa)
                documentos_pdf = loader.load()
                documentos.extend(documentos_pdf)
    return documentos

# ======================================
# ğŸŒ 4. FunciÃ³n para traducir texto de inglÃ©s a espaÃ±ol
# ======================================
from transformers import MarianMTModel, MarianTokenizer
import torch

# Modelo para traducir de inglÃ©s a espaÃ±ol
modelo_trad = "Helsinki-NLP/opus-mt-en-es"
tokenizer_trad = MarianTokenizer.from_pretrained(modelo_trad)
model_trad = MarianMTModel.from_pretrained(modelo_trad)

def traducir_texto(texto, max_length=512):
    oraciones = [texto[i:i+max_length] for i in range(0, len(texto), max_length)]
    resultado = []
    for segmento in oraciones:
        inputs = tokenizer_trad(segmento, return_tensors="pt", truncation=True)
        translated = model_trad.generate(**inputs)
        texto_traducido = tokenizer_trad.decode(translated[0], skip_special_tokens=True)
        resultado.append(texto_traducido)
    return " ".join(resultado)

# ======================================
# ğŸ“„ 5. TraducciÃ³n de documentos en inglÃ©s
# ======================================
def traducir_documentos_en_ingles(ruta):
    documentos = []
    for archivo in os.listdir(ruta):
        if archivo.endswith(".pdf"):
            ruta_completa = os.path.join(ruta, archivo)
            loader = PyMuPDFLoader(ruta_completa)
            docs = loader.load()
            for doc in docs:
                texto_original = doc.page_content
                texto_traducido = traducir_texto(texto_original)
                doc.page_content = texto_traducido
                documentos.append(doc)
    return documentos

# ======================================
# ğŸ“¦ 6. Ejecutamos la carga total del corpus
# ======================================
documentos_es = cargar_documentos_desde_carpeta([carpetas_es])
documentos_en = traducir_documentos_en_ingles(carpeta_en)

documentos = documentos_es + documentos_en

"""CHUNKING (DivisiÃ³n en fragmentos)"""

# ğŸ“Œ VALIDACIONES previas antes del chunking

print("ğŸ” Validando el corpus final...")

# 1. Â¿QuÃ© tipo de objeto es el corpus?
print(f"Tipo de corpus_completo: {type(documentos)}")

# 2. Â¿CuÃ¡ntos documentos contiene?
print(f"ğŸ“š Total de documentos: {len(documentos)}")

# 3. Â¿QuÃ© tipo de objeto es cada documento?
if documentos:
    print(f"Ejemplo de tipo de documento: {type(documentos[0])}")

# 4. Mostrar los primeros 500 caracteres del primer documento
if documentos and hasattr(documentos[0], "page_content"):
    print("\nğŸ“ Vista previa del primer documento:")
    print(documentos[0].page_content[:500])
else:
    print("âš ï¸ No se encontrÃ³ texto en el primer documento.")

"""CHUNKING (DivisiÃ³n en fragmentos)"""

# âœ… Importamos la herramienta para dividir documentos en fragmentos/chunks

#from langchain.text_splitter import RecursiveCharacterTextSplitter
# Cambio a nueva libreria, langchain.text_splitter me daba errores por versiones de langchain
from langchain_text_splitters import RecursiveCharacterTextSplitter


# âœ… Configuramos el splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # Cantidad mÃ¡xima de caracteres por fragmento
    chunk_overlap=50,      # CuÃ¡ntos caracteres se solapan entre fragmentos
    separators=["\n\n", "\n", ".", " ", ""],  # Orden de preferencia para cortar texto
)

# âœ… Aplicar el splitter a todos los documentos
chunks = text_splitter.split_documents(documentos)

from pathlib import Path

# âœ… Enriquecer y simplificar la metadata de cada chunk
for chunk in chunks:
    metadata = chunk.metadata

    # Extraer nombre de archivo limpio
    file_name = Path(metadata.get("file_path", "desconocido")).stem

    # Agregar nombre simple
    metadata["nombre_documento"] = file_name

    # Agregar categorÃ­a manual segÃºn carpeta origen
    if "corpus_base" in metadata.get("file_path", ""):
        metadata["categoria"] = "base_tecnica"
        metadata["idioma"] = "es"
    elif "complementos_tecnicos" in metadata.get("file_path", ""):
        metadata["categoria"] = "complemento"
        metadata["idioma"] = "en"
    else:
        metadata["categoria"] = "otro"
        metadata["idioma"] = "desconocido"

    # Limpiar metadata innecesaria
    for campo in ["producer", "creator", "format", "encryption", "trapped", "moddate", "creationdate", "title", "author", "subject", "keywords"]:
        metadata.pop(campo, None)

# Validar resultados del chunking
print(f"ğŸ“„ Total de chunks generados: {len(chunks)}")
print("\nğŸ“ Primer chunk:")
print(chunks[0].page_content[:500])

# Revisar la metadata de los primeros 3 chunks
for i, chunk in enumerate(chunks[:3]):
    print(f"\nğŸ§¾ Chunk {i+1} - Metadata:")
    print(chunk.metadata)

"""EMBEDDINGS"""

# ==========================
# Paso 3.1: Cargar modelo de embeddings en espaÃ±ol
# ==========================

# Original
# from langchain.embeddings import HuggingFaceEmbeddings
# Nueva
from langchain_huggingface import HuggingFaceEmbeddings

# Definir el modelo de embeddings
modelo_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# ===============================================
# ğŸ§  Paso 3.2: VectorizaciÃ³n de Chunks con ChromaDB
# ===============================================

# Ruta donde se almacenarÃ¡ la base vectorial
persistencia_vectores = "db_vectores"

# Crear base de datos vectorial con los embeddings
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=modelo_embeddings,
    persist_directory="db_vectores"
)

# Guardar la base persistente en disco
chroma_db.persist()

# ValidaciÃ³n visual
print("âœ… Base de datos vectorial creada con Ã©xito y guardada en:", persistencia_vectores)

# ====================================================
# ğŸ§  Paso 5.1: Cargar base vectorial persistente y preparar el Retriever
# ====================================================
# Ya importado
#from langchain_community.vectorstores import Chroma

# ğŸ”„ Ruta donde guardaste la base de datos vectorial
persistencia_vectores = "db_vectores"

# ğŸ—ƒï¸ Cargar la base vectorial persistente desde el disco
chroma_db = Chroma(
    persist_directory=persistencia_vectores,
    embedding_function=modelo_embeddings
)

# ğŸ” Crear un Retriever para realizar bÃºsquedas por similitud
retriever = chroma_db.as_retriever(search_kwargs={"k": 3})

# âœ… ValidaciÃ³n
print("Retriever creado correctamente. Listo para recuperar chunks similares.")

# =============================================
# ğŸŸª Paso 5.2: Consulta de Prueba y RecuperaciÃ³n (SIN RetrievalQA)
# =============================================
# --- Imports actualizados ---
# (Ya estÃ¡n arriba si los cambiaste antes)
# from langchain_community.vectorstores import Chroma # <-- Ya deberÃ­a estar
# from langchain_huggingface import HuggingFaceEmbeddings # <-- Ya deberÃ­a estar (o el nuevo import)
# --- Fin Imports ---

# ğŸ§  Cargamos la base vectorial persistida
## modelo_embeddings ya estaba cargado mas arriba y es compatible con import nuevo

# Si 'modelo_embeddings' fue definido antes como HuggingFaceEmbeddings, se puede reusar.
# Si usaste el nuevo import, asegÃºrate de que 'modelo_embeddings' estÃ© disponible.
# Por ejemplo, si lo definiste asÃ­ antes (con el nuevo import):
# from langchain_huggingface import HuggingFaceEmbeddings
# modelo_embeddings = HuggingFaceEmbeddings(
#     model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# )
# Entonces, 'modelo_embeddings' deberÃ­a estar disponible aquÃ­.

chroma_db = Chroma(
    persist_directory="db_vectores",
    embedding_function=modelo_embeddings
)
# ğŸ” Creamos el retriever (mecanismo de recuperaciÃ³n)
retriever = chroma_db.as_retriever(
    search_type="similarity",  # TambiÃ©n puedes usar "mmr" (Maximal Marginal Relevance)
    search_kwargs={"k": 3}      # NÃºmero de documentos mÃ¡s similares que queremos recuperar
)
# ğŸ§ª Definimos una pregunta de prueba
pregunta_prueba = "Â¿QuÃ© tÃ©cnicas se utilizan en el fracturamiento hidrÃ¡ulico de reservorios no convencionales?"
# Recuperamos los documentos mÃ¡s relevantes usando el retriever directamente

#### GBG - Se intento con esta linea, pero da error, sugiriendo usar una funcion
#### privada que no siempre funciona, y corremos riesgo de que rompa al mandarlo a huggingface
#documentos_recuperados = retriever.get_relevant_documents(pregunta_prueba)

#### Prueba con otro metodo (invoke)
documentos_recuperados = retriever.invoke(pregunta_prueba)

# Mostramos los resultados (esto es parte del "Retrieval", antes de la "Generation")
print("ğŸ“Œ Resultados de la recuperaciÃ³n:")
for i, doc in enumerate(documentos_recuperados, 1):
    print(f"ğŸ”¹ Documento {i}:")
    print(doc.page_content[:500])  # Muestra los primeros 500 caracteres
    print("ğŸ“ Metadata:", doc.metadata)
    print("-" * 80)

# ğŸ§  AquÃ­ termina la parte de "Retrieval" del RAG.
# La parte de "Generation" (usar un LLM para responder la pregunta con el contexto recuperado)
# se harÃ­a en otro paso, por ejemplo, en app.py, usando el retriever y un modelo de lenguaje.
# Por ejemplo, podrÃ­as pasar 'documentos_recuperados' y 'pregunta_prueba' a una cadena RAG allÃ­.
print("Paso 5.2 completado. Se han recuperado los documentos relevantes.")