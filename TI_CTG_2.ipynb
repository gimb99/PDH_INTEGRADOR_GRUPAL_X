{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimb99/PDH_INTEGRADOR_GRUPAL_X/blob/develop/TI_CTG_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Paso 2.1 ‚Äì Ingesta de documentos PDF y traducci√≥n autom√°tica\n",
        "\n",
        "Este paso carga los documentos t√©cnicos desde el disco local.\n",
        "Se leen PDFs tanto en espa√±ol como en ingl√©s.  \n",
        "Los documentos en ingl√©s se traducen autom√°ticamente al espa√±ol con un modelo de Hugging Face.\n",
        "El resultado final es un corpus unificado en espa√±ol que se utilizar√° en los siguientes pasos del sistema RAG.\n"
      ],
      "metadata": {
        "id": "BGfB83KAN0sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Paso 2.1 ‚Äì Ingesta y traducci√≥n autom√°tica de documentos PDF\n",
        "\n",
        "# ========================\n",
        "# ‚úÖ Librer√≠as necesarias\n",
        "# ========================\n",
        "!pip install --upgrade langchain langchain-community langchain-core langchain-text-splitters transformers pypdf pymupdf  # Upgrade core libraries\n",
        "!pip install -U huggingface_hub# Install compatible huggingface_hub\n",
        "from langchain_community.document_loaders import PyMuPDFLoader  # Para cargar PDFs\n",
        "\n",
        "from transformers import pipeline                                # Para traducir texto\n",
        "import os                                                        # Para acceder a archivos en disco\n",
        "\n",
        "\n",
        "!pip install chromadb\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8Hbzli7iOY3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# ‚úÖ Modelo de traducci√≥n\n",
        "# ========================\n",
        "# Este modelo traduce texto de ingl√©s a espa√±ol usando Hugging Face\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n"
      ],
      "metadata": {
        "id": "sSw55fM0O0T2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 2.1 ‚Äì INGESTA DE DOCUMENTOS: Carga de archivos PDF desde Google Drive\n",
        "\n",
        "# ======================================\n",
        "# üîß 1. Montamos Google Drive (solo en Colab)\n",
        "# ======================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "HVYiDBSAYmGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìÅ 2. Definimos las carpetas del corpus\n",
        "# ======================================\n",
        "carpetas_es = \"/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base\"\n",
        "\n",
        "carpeta_en = \"/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/complementos_tecnicos\"\n"
      ],
      "metadata": {
        "id": "8020KBEYYovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìö 3. Funci√≥n para cargar documentos PDF\n",
        "# ======================================\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "import os\n",
        "\n",
        "def cargar_documentos_desde_carpeta(rutas_carpeta):\n",
        "    documentos = []\n",
        "    for ruta in rutas_carpeta:\n",
        "        for archivo in os.listdir(ruta):\n",
        "            if archivo.endswith(\".pdf\"):\n",
        "                ruta_completa = os.path.join(ruta, archivo)\n",
        "                loader = PyMuPDFLoader(ruta_completa)\n",
        "                documentos_pdf = loader.load()\n",
        "                documentos.extend(documentos_pdf)\n",
        "    return documentos"
      ],
      "metadata": {
        "id": "bHhC0w_ZY3NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üåç 4. Funci√≥n para traducir texto de ingl√©s a espa√±ol\n",
        "# ======================================\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "# Modelo para traducir de ingl√©s a espa√±ol\n",
        "modelo_trad = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer_trad = MarianTokenizer.from_pretrained(modelo_trad)\n",
        "model_trad = MarianMTModel.from_pretrained(modelo_trad)\n",
        "\n",
        "def traducir_texto(texto, max_length=512):\n",
        "    oraciones = [texto[i:i+max_length] for i in range(0, len(texto), max_length)]\n",
        "    resultado = []\n",
        "    for segmento in oraciones:\n",
        "        inputs = tokenizer_trad(segmento, return_tensors=\"pt\", truncation=True)\n",
        "        translated = model_trad.generate(**inputs)\n",
        "        texto_traducido = tokenizer_trad.decode(translated[0], skip_special_tokens=True)\n",
        "        resultado.append(texto_traducido)\n",
        "    return \" \".join(resultado)\n"
      ],
      "metadata": {
        "id": "yWqa--6Pbc3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìÑ 5. Traducci√≥n de documentos en ingl√©s\n",
        "# ======================================\n",
        "def traducir_documentos_en_ingles(ruta):\n",
        "    documentos = []\n",
        "    for archivo in os.listdir(ruta):\n",
        "        if archivo.endswith(\".pdf\"):\n",
        "            ruta_completa = os.path.join(ruta, archivo)\n",
        "            loader = PyMuPDFLoader(ruta_completa)\n",
        "            docs = loader.load()\n",
        "            for doc in docs:\n",
        "                texto_original = doc.page_content\n",
        "                texto_traducido = traducir_texto(texto_original)\n",
        "                doc.page_content = texto_traducido\n",
        "                documentos.append(doc)\n",
        "    return documentos\n"
      ],
      "metadata": {
        "id": "srn16Sz0bkBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üì¶ 6. Ejecutamos la carga total del corpus\n",
        "# ======================================\n",
        "documentos_es = cargar_documentos_desde_carpeta([carpetas_es])\n",
        "documentos_en = traducir_documentos_en_ingles(carpeta_en)\n",
        "\n",
        "documentos = documentos_es + documentos_en"
      ],
      "metadata": {
        "id": "-7NbuI4ubsra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHUNKING (Divisi√≥n en fragmentos)"
      ],
      "metadata": {
        "id": "VQfUYF8NB8ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå VALIDACIONES previas antes del chunking\n",
        "\n",
        "print(\"üîé Validando el corpus final...\")\n",
        "\n",
        "# 1. ¬øQu√© tipo de objeto es el corpus?\n",
        "print(f\"Tipo de corpus_completo: {type(documentos)}\")\n",
        "\n",
        "# 2. ¬øCu√°ntos documentos contiene?\n",
        "print(f\"üìö Total de documentos: {len(documentos)}\")\n",
        "\n",
        "# 3. ¬øQu√© tipo de objeto es cada documento?\n",
        "if documentos:\n",
        "    print(f\"Ejemplo de tipo de documento: {type(documentos[0])}\")\n",
        "\n",
        "# 4. Mostrar los primeros 500 caracteres del primer documento\n",
        "if documentos and hasattr(documentos[0], \"page_content\"):\n",
        "    print(\"\\nüìù Vista previa del primer documento:\")\n",
        "    print(documentos[0].page_content[:500])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ texto en el primer documento.\")\n"
      ],
      "metadata": {
        "id": "TkXoQiKE7x4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHUNKING (Divisi√≥n en fragmentos)"
      ],
      "metadata": {
        "id": "Kdy05qXM9Vwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Importamos la herramienta para dividir documentos en fragmentos/chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "_kh3hr9g9ejm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Configuramos el splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,        # Cantidad m√°xima de caracteres por fragmento\n",
        "    chunk_overlap=50,      # Cu√°ntos caracteres se solapan entre fragmentos\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Orden de preferencia para cortar texto\n",
        ")\n"
      ],
      "metadata": {
        "id": "w1PSZcKi9wSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Aplicar el splitter a todos los documentos\n",
        "chunks = text_splitter.split_documents(documentos)\n"
      ],
      "metadata": {
        "id": "8y8TF6zN-FUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ‚úÖ Enriquecer y simplificar la metadata de cada chunk\n",
        "for chunk in chunks:\n",
        "    metadata = chunk.metadata\n",
        "\n",
        "    # Extraer nombre de archivo limpio\n",
        "    file_name = Path(metadata.get(\"file_path\", \"desconocido\")).stem\n",
        "\n",
        "    # Agregar nombre simple\n",
        "    metadata[\"nombre_documento\"] = file_name\n",
        "\n",
        "    # Agregar categor√≠a manual seg√∫n carpeta origen\n",
        "    if \"corpus_base\" in metadata.get(\"file_path\", \"\"):\n",
        "        metadata[\"categoria\"] = \"base_tecnica\"\n",
        "        metadata[\"idioma\"] = \"es\"\n",
        "    elif \"complementos_tecnicos\" in metadata.get(\"file_path\", \"\"):\n",
        "        metadata[\"categoria\"] = \"complemento\"\n",
        "        metadata[\"idioma\"] = \"en\"\n",
        "    else:\n",
        "        metadata[\"categoria\"] = \"otro\"\n",
        "        metadata[\"idioma\"] = \"desconocido\"\n",
        "\n",
        "    # Limpiar metadata innecesaria\n",
        "    for campo in [\"producer\", \"creator\", \"format\", \"encryption\", \"trapped\", \"moddate\", \"creationdate\", \"title\", \"author\", \"subject\", \"keywords\"]:\n",
        "        metadata.pop(campo, None)\n"
      ],
      "metadata": {
        "id": "XqF81NJgAYqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar resultados del chunking\n",
        "print(f\"üìÑ Total de chunks generados: {len(chunks)}\")\n",
        "print(\"\\nüìù Primer chunk:\")\n",
        "print(chunks[0].page_content[:500])\n"
      ],
      "metadata": {
        "id": "P7DfwzkT-Kzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar la metadata de los primeros 3 chunks\n",
        "for i, chunk in enumerate(chunks[:3]):\n",
        "    print(f\"\\nüßæ Chunk {i+1} - Metadata:\")\n",
        "    print(chunk.metadata)\n"
      ],
      "metadata": {
        "id": "e8i9vDVf_sGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMBEDDINGS"
      ],
      "metadata": {
        "id": "EUj-e7pCB_1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Paso 3.1: Cargar modelo de embeddings en espa√±ol\n",
        "# ==========================\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings # Updated import\n",
        "\n",
        "# Definir el modelo de embeddings\n",
        "modelo_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "1FhfnE1bCEq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================================\n",
        "# üß† Paso 3.2: Vectorizaci√≥n de Chunks con ChromaDB\n",
        "# ===============================================\n",
        "\n",
        "# Ruta donde se almacenar√° la base vectorial\n",
        "persistencia_vectores = \"db_vectores\"\n",
        "\n",
        "# Crear base de datos vectorial con los embeddings\n",
        "chroma_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=modelo_embeddings,\n",
        "    persist_directory=\"db_vectores\"\n",
        ")\n",
        "\n",
        "# Guardar la base persistente en disco\n",
        "chroma_db.persist()\n",
        "\n",
        "# Validaci√≥n visual\n",
        "print(\"‚úÖ Base de datos vectorial creada con √©xito y guardada en:\", persistencia_vectores)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1oPdRJ7bHHY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# üß† Paso 5.1: Cargar base vectorial persistente y preparar el Retriever\n",
        "# ====================================================\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# üîÑ Ruta donde guardaste la base de datos vectorial\n",
        "persistencia_vectores = \"db_vectores\"\n",
        "\n",
        "# üóÉÔ∏è Cargar la base vectorial persistente desde el disco\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=persistencia_vectores,\n",
        "    embedding_function=modelo_embeddings\n",
        ")\n",
        "\n",
        "# üîç Crear un Retriever para realizar b√∫squedas por similitud\n",
        "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# ‚úÖ Validaci√≥n\n",
        "print(\"‚úÖ Retriever creado correctamente. Listo para recuperar chunks similares.\")\n"
      ],
      "metadata": {
        "id": "C2_XUcDDOLsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# ∆á Paso 5.2: Consulta de Prueba y Recuperaci√≥n\n",
        "# =============================================\n",
        "\n",
        "# ‚úÖ Imports necesarios para este paso\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# ∆á Cargamos la base vectorial persistida\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"db_vectores\",\n",
        "    embedding_function=modelo_embeddings\n",
        ")\n",
        "\n",
        "# ∆á Creamos el retriever (mecanismo de recuperaci√≥n)\n",
        "retriever = chroma_db.as_retriever(\n",
        "    search_type=\"similarity\",  # Tambi√©n puedes usar \"mmr\" (Maximal Marginal Relevance)\n",
        "    search_kwargs={\"k\": 3}      # N√∫mero de documentos m√°s similares que queremos recuperar\n",
        ")\n",
        "\n",
        "# ∆á Definimos una pregunta de prueba\n",
        "pregunta_prueba = \"¬øQu√© es un reservorio no convencional?\"\n",
        "\n",
        "# ∆á Recuperamos los documentos m√°s relevantes\n",
        "resultados = retriever.get_relevant_documents(pregunta_prueba)\n",
        "\n",
        "# ∆á Mostramos los resultados\n",
        "print(\"∆í Resultados de la recuperaci√≥n:\\n\")\n",
        "for i, doc in enumerate(resultados, 1):\n",
        "    print(f\"∆í Documento {i}:\")\n",
        "    print(doc.page_content[:500])  # Muestra los primeros 500 caracteres\n",
        "    print(\"∆í Metadata:\", doc.metadata)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "v3jq_bYTOuRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso previo: cargar el modelo de lenguaje LLM desde Hugging Face\n",
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# ‚ö†Ô∏è Agrega tu token personal de Hugging Face\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"üîë Ingresa tu token de Hugging Face: \")\n",
        "\n",
        "# ‚úÖ Carga del modelo (puedes cambiar por otro compatible)\n",
        "llm_model = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",  # Otro modelo posible: \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "    model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "# ‚úÖ Ahora s√≠ puedes crear tu cadena RAG\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_model,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt_personalizado}\n",
        ")\n"
      ],
      "metadata": {
        "id": "vZSSlsIPpcl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}