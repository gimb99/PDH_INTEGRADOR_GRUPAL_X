{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimb99/PDH_INTEGRADOR_GRUPAL_X/blob/develop/ti_ctg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Paso 2.1 ‚Äì Ingesta de documentos PDF y traducci√≥n autom√°tica\n",
        "\n",
        "Este paso carga los documentos t√©cnicos desde el disco local.\n",
        "Se leen PDFs tanto en espa√±ol como en ingl√©s.  \n",
        "Los documentos en ingl√©s se traducen autom√°ticamente al espa√±ol con un modelo de Hugging Face.\n",
        "El resultado final es un corpus unificado en espa√±ol que se utilizar√° en los siguientes pasos del sistema RAG.\n"
      ],
      "metadata": {
        "id": "BGfB83KAN0sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Paso 2.1 ‚Äì Ingesta y traducci√≥n autom√°tica de documentos PDF\n",
        "\n",
        "# ========================\n",
        "# ‚úÖ Librer√≠as necesarias\n",
        "# ========================\n",
        "!pip install --upgrade langchain langchain-community langchain-core langchain-text-splitters transformers pypdf pymupdf  # Upgrade core libraries\n",
        "!pip install -U huggingface_hub# Install compatible huggingface_hub\n",
        "from langchain_community.document_loaders import PyMuPDFLoader  # Para cargar PDFs\n",
        "\n",
        "from transformers import pipeline                                # Para traducir texto\n",
        "import os                                                        # Para acceder a archivos en disco\n",
        "\n",
        "\n",
        "!pip install chromadb\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8Hbzli7iOY3h",
        "outputId": "31b9bda0-dafc-4a0b-f768-8e147fa02105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.4)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.2.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "\u001b[31mERROR: Invalid requirement: 'huggingface_hub#': Expected end or semicolon (after name and no valid version specifier)\n",
            "    huggingface_hub#\n",
            "                   ^\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# ‚úÖ Modelo de traducci√≥n\n",
        "# ========================\n",
        "# Este modelo traduce texto de ingl√©s a espa√±ol usando Hugging Face\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n"
      ],
      "metadata": {
        "id": "sSw55fM0O0T2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3d0f53b5-2fe1-494b-9e79-bb724d9cc00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 2.1 ‚Äì INGESTA DE DOCUMENTOS: Carga de archivos PDF desde Google Drive\n",
        "\n",
        "# ======================================\n",
        "# üîß 1. Montamos Google Drive (solo en Colab)\n",
        "# ======================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "HVYiDBSAYmGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679286be-5cf8-4f10-8109-43d265c4f342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìÅ 2. Definimos las carpetas del corpus\n",
        "# ======================================\n",
        "carpetas_es = \"/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base\"\n",
        "\n",
        "carpeta_en = \"/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/complementos_tecnicos\"\n"
      ],
      "metadata": {
        "id": "8020KBEYYovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìö 3. Funci√≥n para cargar documentos PDF\n",
        "# ======================================\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "import os\n",
        "\n",
        "def cargar_documentos_desde_carpeta(rutas_carpeta):\n",
        "    documentos = []\n",
        "    for ruta in rutas_carpeta:\n",
        "        for archivo in os.listdir(ruta):\n",
        "            if archivo.endswith(\".pdf\"):\n",
        "                ruta_completa = os.path.join(ruta, archivo)\n",
        "                loader = PyMuPDFLoader(ruta_completa)\n",
        "                documentos_pdf = loader.load()\n",
        "                documentos.extend(documentos_pdf)\n",
        "    return documentos"
      ],
      "metadata": {
        "id": "bHhC0w_ZY3NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üåç 4. Funci√≥n para traducir texto de ingl√©s a espa√±ol\n",
        "# ======================================\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "# Modelo para traducir de ingl√©s a espa√±ol\n",
        "modelo_trad = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer_trad = MarianTokenizer.from_pretrained(modelo_trad)\n",
        "model_trad = MarianMTModel.from_pretrained(modelo_trad)\n",
        "\n",
        "def traducir_texto(texto, max_length=512):\n",
        "    oraciones = [texto[i:i+max_length] for i in range(0, len(texto), max_length)]\n",
        "    resultado = []\n",
        "    for segmento in oraciones:\n",
        "        inputs = tokenizer_trad(segmento, return_tensors=\"pt\", truncation=True)\n",
        "        translated = model_trad.generate(**inputs)\n",
        "        texto_traducido = tokenizer_trad.decode(translated[0], skip_special_tokens=True)\n",
        "        resultado.append(texto_traducido)\n",
        "    return \" \".join(resultado)\n"
      ],
      "metadata": {
        "id": "yWqa--6Pbc3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d61604-8ac8-4c1d-94f4-cfff3c05c353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üìÑ 5. Traducci√≥n de documentos en ingl√©s\n",
        "# ======================================\n",
        "def traducir_documentos_en_ingles(ruta):\n",
        "    documentos = []\n",
        "    for archivo in os.listdir(ruta):\n",
        "        if archivo.endswith(\".pdf\"):\n",
        "            ruta_completa = os.path.join(ruta, archivo)\n",
        "            loader = PyMuPDFLoader(ruta_completa)\n",
        "            docs = loader.load()\n",
        "            for doc in docs:\n",
        "                texto_original = doc.page_content\n",
        "                texto_traducido = traducir_texto(texto_original)\n",
        "                doc.page_content = texto_traducido\n",
        "                documentos.append(doc)\n",
        "    return documentos\n"
      ],
      "metadata": {
        "id": "srn16Sz0bkBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# üì¶ 6. Ejecutamos la carga total del corpus\n",
        "# ======================================\n",
        "documentos_es = cargar_documentos_desde_carpeta([carpetas_es])\n",
        "documentos_en = traducir_documentos_en_ingles(carpeta_en)\n",
        "\n",
        "documentos = documentos_es + documentos_en"
      ],
      "metadata": {
        "id": "-7NbuI4ubsra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHUNKING (Divisi√≥n en fragmentos)"
      ],
      "metadata": {
        "id": "VQfUYF8NB8ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå VALIDACIONES previas antes del chunking\n",
        "\n",
        "print(\"üîé Validando el corpus final...\")\n",
        "\n",
        "# 1. ¬øQu√© tipo de objeto es el corpus?\n",
        "print(f\"Tipo de corpus_completo: {type(documentos)}\")\n",
        "\n",
        "# 2. ¬øCu√°ntos documentos contiene?\n",
        "print(f\"üìö Total de documentos: {len(documentos)}\")\n",
        "\n",
        "# 3. ¬øQu√© tipo de objeto es cada documento?\n",
        "if documentos:\n",
        "    print(f\"Ejemplo de tipo de documento: {type(documentos[0])}\")\n",
        "\n",
        "# 4. Mostrar los primeros 500 caracteres del primer documento\n",
        "if documentos and hasattr(documentos[0], \"page_content\"):\n",
        "    print(\"\\nüìù Vista previa del primer documento:\")\n",
        "    print(documentos[0].page_content[:500])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ texto en el primer documento.\")\n"
      ],
      "metadata": {
        "id": "TkXoQiKE7x4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2052ed7-efc2-4a1a-f6fa-0661d84adc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Validando el corpus final...\n",
            "Tipo de corpus_completo: <class 'list'>\n",
            "üìö Total de documentos: 420\n",
            "Ejemplo de tipo de documento: <class 'langchain_core.documents.base.Document'>\n",
            "\n",
            "üìù Vista previa del primer documento:\n",
            "Manual de Producci√≥n \n",
            "Upstream\n",
            "Tomos 1, 2 y 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHUNKING (Divisi√≥n en fragmentos)"
      ],
      "metadata": {
        "id": "Kdy05qXM9Vwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Importamos la herramienta para dividir documentos en fragmentos/chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "_kh3hr9g9ejm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Configuramos el splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,        # Cantidad m√°xima de caracteres por fragmento\n",
        "    chunk_overlap=50,      # Cu√°ntos caracteres se solapan entre fragmentos\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Orden de preferencia para cortar texto\n",
        ")\n"
      ],
      "metadata": {
        "id": "w1PSZcKi9wSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Aplicar el splitter a todos los documentos\n",
        "chunks = text_splitter.split_documents(documentos)\n"
      ],
      "metadata": {
        "id": "8y8TF6zN-FUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ‚úÖ Enriquecer y simplificar la metadata de cada chunk\n",
        "for chunk in chunks:\n",
        "    metadata = chunk.metadata\n",
        "\n",
        "    # Extraer nombre de archivo limpio\n",
        "    file_name = Path(metadata.get(\"file_path\", \"desconocido\")).stem\n",
        "\n",
        "    # Agregar nombre simple\n",
        "    metadata[\"nombre_documento\"] = file_name\n",
        "\n",
        "    # Agregar categor√≠a manual seg√∫n carpeta origen\n",
        "    if \"corpus_base\" in metadata.get(\"file_path\", \"\"):\n",
        "        metadata[\"categoria\"] = \"base_tecnica\"\n",
        "        metadata[\"idioma\"] = \"es\"\n",
        "    elif \"complementos_tecnicos\" in metadata.get(\"file_path\", \"\"):\n",
        "        metadata[\"categoria\"] = \"complemento\"\n",
        "        metadata[\"idioma\"] = \"en\"\n",
        "    else:\n",
        "        metadata[\"categoria\"] = \"otro\"\n",
        "        metadata[\"idioma\"] = \"desconocido\"\n",
        "\n",
        "    # Limpiar metadata innecesaria\n",
        "    for campo in [\"producer\", \"creator\", \"format\", \"encryption\", \"trapped\", \"moddate\", \"creationdate\", \"title\", \"author\", \"subject\", \"keywords\"]:\n",
        "        metadata.pop(campo, None)\n"
      ],
      "metadata": {
        "id": "XqF81NJgAYqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar resultados del chunking\n",
        "print(f\"üìÑ Total de chunks generados: {len(chunks)}\")\n",
        "print(\"\\nüìù Primer chunk:\")\n",
        "print(chunks[0].page_content[:500])\n"
      ],
      "metadata": {
        "id": "P7DfwzkT-Kzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6691f364-46b0-450e-94d4-aa73b2eaed6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Total de chunks generados: 2747\n",
            "\n",
            "üìù Primer chunk:\n",
            "Manual de Producci√≥n \n",
            "Upstream\n",
            "Tomos 1, 2 y 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar la metadata de los primeros 3 chunks\n",
        "for i, chunk in enumerate(chunks[:3]):\n",
        "    print(f\"\\nüßæ Chunk {i+1} - Metadata:\")\n",
        "    print(chunk.metadata)\n"
      ],
      "metadata": {
        "id": "e8i9vDVf_sGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33dbce0-42a6-4b52-abba-c44de53d5e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßæ Chunk 1 - Metadata:\n",
            "{'source': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'file_path': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'total_pages': 393, 'modDate': \"D:20130808171109-03'00'\", 'creationDate': \"D:20130806131834-03'00'\", 'page': 0, 'nombre_documento': 'Manual de ProduccioÃÅn Upstream YPF', 'categoria': 'base_tecnica', 'idioma': 'es'}\n",
            "\n",
            "üßæ Chunk 2 - Metadata:\n",
            "{'source': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'file_path': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'total_pages': 393, 'modDate': \"D:20130808171109-03'00'\", 'creationDate': \"D:20130806131834-03'00'\", 'page': 1, 'nombre_documento': 'Manual de ProduccioÃÅn Upstream YPF', 'categoria': 'base_tecnica', 'idioma': 'es'}\n",
            "\n",
            "üßæ Chunk 3 - Metadata:\n",
            "{'source': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'file_path': '/content/drive/MyDrive/ProcHabla/Trabajo Integrador/corpus/corpus_base/Manual de ProduccioÃÅn Upstream YPF.pdf', 'total_pages': 393, 'modDate': \"D:20130808171109-03'00'\", 'creationDate': \"D:20130806131834-03'00'\", 'page': 2, 'nombre_documento': 'Manual de ProduccioÃÅn Upstream YPF', 'categoria': 'base_tecnica', 'idioma': 'es'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMBEDDINGS"
      ],
      "metadata": {
        "id": "EUj-e7pCB_1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Paso 3.1: Cargar modelo de embeddings en espa√±ol\n",
        "# ==========================\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings # Updated import\n",
        "\n",
        "# Definir el modelo de embeddings\n",
        "modelo_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "1FhfnE1bCEq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53c73db-9843-42cc-a652-d5c6fc00d2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3578164995.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  modelo_embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================================\n",
        "# üß† Paso 3.2: Vectorizaci√≥n de Chunks con ChromaDB\n",
        "# ===============================================\n",
        "\n",
        "# Ruta donde se almacenar√° la base vectorial\n",
        "persistencia_vectores = \"db_vectores\"\n",
        "\n",
        "# Crear base de datos vectorial con los embeddings\n",
        "chroma_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=modelo_embeddings,\n",
        "    persist_directory=\"db_vectores\"\n",
        ")\n",
        "\n",
        "# Guardar la base persistente en disco\n",
        "chroma_db.persist()\n",
        "\n",
        "# Validaci√≥n visual\n",
        "print(\"‚úÖ Base de datos vectorial creada con √©xito y guardada en:\", persistencia_vectores)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1oPdRJ7bHHY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d367c54-7865-4a42-8d73-9decb3f30b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Base de datos vectorial creada con √©xito y guardada en: db_vectores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1970422414.py:16: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  chroma_db.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HbzliY3h"
      },
      "source": [
        "# üìå Paso 2.1 ‚Äì Ingesta y traducci√≥n autom√°tica de documentos PDF\n",
        "\n",
        "# ========================\n",
        "# ‚úÖ Librer√≠as necesarias\n",
        "# ========================\n",
        "!pip install --upgrade langchain langchain-community langchain-core langchain-text-splitters transformers pypdf pymupdf  # Upgrade core libraries\n",
        "!pip install -U huggingface_hub langchain-huggingface # Install compatible huggingface_hub and langchain-huggingface\n",
        "from langchain_community.document_loaders import PyMuPDFLoader  # Para cargar PDFs\n",
        "\n",
        "from transformers import pipeline                                # Para traducir texto\n",
        "import os                                                        # Para acceder a archivos en disco\n",
        "\n",
        "\n",
        "!pip install chromadb\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# üß† Paso 5.1: Cargar base vectorial persistente y preparar el Retriever\n",
        "# ====================================================\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# üîÑ Ruta donde guardaste la base de datos vectorial\n",
        "persistencia_vectores = \"db_vectores\"\n",
        "\n",
        "# üóÉÔ∏è Cargar la base vectorial persistente desde el disco\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=persistencia_vectores,\n",
        "    embedding_function=modelo_embeddings\n",
        ")\n",
        "\n",
        "# üîç Crear un Retriever para realizar b√∫squedas por similitud\n",
        "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# ‚úÖ Validaci√≥n\n",
        "print(\"‚úÖ Retriever creado correctamente. Listo para recuperar chunks similares.\")\n"
      ],
      "metadata": {
        "id": "C2_XUcDDOLsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb603cd-e2d2-4ff7-8b5d-09a4d19ab0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Retriever creado correctamente. Listo para recuperar chunks similares.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1341945274.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  chroma_db = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# üü™ Paso 5.2: Consulta de Prueba y Recuperaci√≥n\n",
        "# =============================================\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# üß† Cargamos la base vectorial persistida\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"db_vectores\",\n",
        "    embedding_function=modelo_embeddings\n",
        ")\n",
        "\n",
        "# üîç Creamos el retriever (mecanismo de recuperaci√≥n)\n",
        "retriever = chroma_db.as_retriever(\n",
        "    search_type=\"similarity\",  # Tambi√©n puedes usar \"mmr\" (Maximal Marginal Relevance)\n",
        "    search_kwargs={\"k\": 3}      # N√∫mero de documentos m√°s similares que queremos recuperar\n",
        ")\n",
        "\n",
        "# üß™ Definimos una pregunta de prueba\n",
        "pregunta_prueba = \"¬øQu√© es un reservorio no convencional?\"\n",
        "\n",
        "# üîÑ Recuperamos los documentos m√°s relevantes\n",
        "resultados = retriever.get_relevant_documents(pregunta_prueba)\n",
        "\n",
        "# üñ®Ô∏è Mostramos los resultados\n",
        "print(\"üìå Resultados de la recuperaci√≥n:\\n\")\n",
        "for i, doc in enumerate(resultados, 1):\n",
        "    print(f\"üîπ Documento {i}:\")\n",
        "    print(doc.page_content[:500])  # Muestra los primeros 500 caracteres\n",
        "    print(\"üìé Metadata:\", doc.metadata)\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "v3jq_bYTOuRz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "7ad70a6b-2078-4937-f007-6302e583aad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.chains'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4236404487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}